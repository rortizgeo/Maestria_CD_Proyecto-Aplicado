{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f75e564d",
   "metadata": {},
   "source": [
    "\n",
    "Pipeline completo para la predicción del crecimiento de datos de biodiversidad en GBIF.\n",
    "\n",
    "Este script implementa el flujo de trabajo de principio a fin para modelar datos de panel\n",
    "de series temporales, incluyendo:\n",
    "1.  Carga y limpieza de datos.\n",
    "2.  Ingeniería de características temporales (lags y ventanas móviles).\n",
    "3.  Un marco de validación cruzada robusto para series de tiempo (ventana expansiva).\n",
    "4.  Preprocesamiento (imputación y escalado) dentro del bucle de validación para evitar fuga de datos.\n",
    "5.  Entrenamiento y evaluación comparativa de cuatro modelos:\n",
    "    - SARIMAX (línea base estadística, ajustado por país).\n",
    "    - Random Forest.\n",
    "    - XGBoost.\n",
    "    - Red Neuronal LSTM (para modelado secuencial).\n",
    "6.  Selección del mejor modelo basado en métricas de rendimiento (MAE, RMSE, R²).\n",
    "7.  Reentrenamiento del modelo final y generación de pronósticos para Colombia hasta 2030\n",
    "    bajo dos escenarios de políticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d92da8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. IMPORTACIÓN DE LIBRERÍAS Y CONFIGURACIÓN INICIAL\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Preprocesamiento y modelado de Scikit-Learn\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Modelos especializados\n",
    "import xgboost as xgb\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# Modelado de Deep Learning con TensorFlow/Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Configuraciones generales\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (15, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6df615b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2. CARGA Y LIMPIEZA INICIAL DE DATOS\n",
    "# =============================================================================\n",
    "print(\"Paso 1: Cargando y limpiando los datos...\")\n",
    "\n",
    "# Cargar el conjunto de datos\n",
    "try:\n",
    "    df = pd.read_csv('Data_final.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'Data_final.csv' no encontrado. Asegúrate de que el archivo esté en el mismo directorio.\")\n",
    "    exit()\n",
    "\n",
    "# Limpieza básica y selección del marco temporal\n",
    "df['year'] = pd.to_numeric(df['year'], errors='coerce')\n",
    "df.dropna(subset=['year'], inplace=True)\n",
    "df['year'] = df['year'].astype(int)\n",
    "df = df[(df['year'] >= 2007) & (df['year'] <= 2022)].copy()\n",
    "\n",
    "# Convertir variables categóricas a numéricas para los modelos\n",
    "df['gbif_member'] = df['gbif_member'].apply(lambda x: 1 if x == 'Sí' else 0)\n",
    "df['ogp_membership'] = df['ogp_membership'].apply(lambda x: 1 if x == 'Sí' else 0)\n",
    "\n",
    "# Rellenar NaNs en la variable objetivo con 0, asumiendo que NaN significa sin publicaciones\n",
    "df['occurrenceCount_publisher'].fillna(0, inplace=True)\n",
    "\n",
    "# Ordenar los datos, crucial para el manejo de series de tiempo\n",
    "df.sort_values(by=['country', 'year'], inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"Datos cargados y limpiados. Rango de años: 2007-2022.\")\n",
    "print(f\"Dimensiones del DataFrame: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cadacbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3. INGENIERÍA DE CARACTERÍSTICAS TEMPORALES\n",
    "# =============================================================================\n",
    "print(\"\\nPaso 2: Realizando ingeniería de características temporales...\")\n",
    "\n",
    "def create_temporal_features(data):\n",
    "    \"\"\"\n",
    "    Crea características de retardo (lag) y de ventana móvil para un DataFrame de panel.\n",
    "    Las operaciones se agrupan por país para evitar la fuga de datos entre series.\n",
    "    \"\"\"\n",
    "    data_copy = data.copy()\n",
    "    \n",
    "    # Lista de características para aplicar transformaciones\n",
    "    features_to_lag =\n",
    "    \n",
    "    for feature in features_to_lag:\n",
    "        # Crear características de retardo (lag)\n",
    "        data_copy[f'{feature}_lag_1'] = data_copy.groupby('country')[feature].shift(1)\n",
    "        \n",
    "        # Crear características de ventana móvil\n",
    "        data_copy[f'{feature}_rolling_mean_3'] = data_copy.groupby('country')[feature].rolling(window=3, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "        data_copy[f'{feature}_rolling_std_3'] = data_copy.groupby('country')[feature].rolling(window=3, min_periods=1).std().reset_index(level=0, drop=True)\n",
    "\n",
    "    return data_copy\n",
    "\n",
    "df_featured = create_temporal_features(df)\n",
    "\n",
    "# Rellenar NaNs generados por las transformaciones iniciales (ej. primer lag)\n",
    "df_featured.fillna(method='bfill', inplace=True)\n",
    "\n",
    "print(\"Ingeniería de características completada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfbcc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4. PREPARACIÓN PARA EL MODELADO Y VALIDACIÓN\n",
    "# =============================================================================\n",
    "print(\"\\nPaso 3: Preparando el marco de validación y los datos para el modelado...\")\n",
    "\n",
    "# Definir variables predictoras (X) y objetivo (y)\n",
    "TARGET = 'occurrenceCount_publisher'\n",
    "# Excluir identificadores y variables que podrían causar fuga de datos directa\n",
    "features =]\n",
    "\n",
    "X = df_featured[features]\n",
    "y = df_featured\n",
    "\n",
    "# Configurar la validación cruzada para series de tiempo\n",
    "# Se divide por año para mantener la estructura de panel intacta en cada pliegue\n",
    "unique_years = X['year'].unique()\n",
    "n_splits = 5 # Usaremos 5 pliegues para la validación\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Diccionario para almacenar los resultados de cada modelo\n",
    "results = {\n",
    "    'SARIMAX':,\n",
    "    'RandomForest':,\n",
    "    'XGBoost':,\n",
    "    'LSTM':\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4ed319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5. BUCLE DE ENTRENAMIENTO Y EVALUACIÓN DE MODELOS\n",
    "# =============================================================================\n",
    "print(\"\\nPaso 4: Iniciando el bucle de entrenamiento y validación de modelos...\")\n",
    "\n",
    "for fold, (train_year_idx, test_year_idx) in enumerate(tscv.split(unique_years)):\n",
    "    print(f\"\\n===== FOLD {fold + 1}/{n_splits} =====\")\n",
    "    \n",
    "    # Identificar los años de entrenamiento y prueba\n",
    "    train_years = unique_years[train_year_idx]\n",
    "    test_years = unique_years[test_year_idx]\n",
    "    print(f\"Años de entrenamiento: {train_years.min()} - {train_years.max()}\")\n",
    "    print(f\"Años de prueba: {test_years.min()} - {test_years.max()}\")\n",
    "\n",
    "    # Dividir los datos en entrenamiento y prueba según los años\n",
    "    train_indices = X[X['year'].isin(train_years)].index\n",
    "    test_indices = X[X['year'].isin(test_years)].index\n",
    "    \n",
    "    X_train, X_test = X.loc[train_indices], X.loc[test_indices]\n",
    "    y_train, y_test = y.loc[train_indices], y.loc[test_indices]\n",
    "\n",
    "    # --- Preprocesamiento DENTRO del bucle para evitar fuga de datos ---\n",
    "    # Guardar los nombres de las columnas y los países para después\n",
    "    X_train_countries = X_train['country']\n",
    "    X_test_countries = X_test['country']\n",
    "    X_train_columns = X_train.drop(columns=['country']).columns\n",
    "    \n",
    "    # Separar 'country' antes de imputar y escalar\n",
    "    X_train_numeric = X_train.drop(columns=['country'])\n",
    "    X_test_numeric = X_test.drop(columns=['country'])\n",
    "\n",
    "    # Imputación de valores faltantes\n",
    "    imputer = IterativeImputer(max_iter=10, random_state=42)\n",
    "    X_train_imputed = imputer.fit_transform(X_train_numeric)\n",
    "    X_test_imputed = imputer.transform(X_test_numeric)\n",
    "\n",
    "    # Escalado de características\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "    X_test_scaled = scaler.transform(X_test_imputed)\n",
    "    \n",
    "    # Convertir de nuevo a DataFrame para facilidad de uso\n",
    "    X_train_processed = pd.DataFrame(X_train_scaled, columns=X_train_columns, index=X_train.index)\n",
    "    X_test_processed = pd.DataFrame(X_test_scaled, columns=X_train_columns, index=X_test.index)\n",
    "    \n",
    "    # Añadir de nuevo la columna 'country'\n",
    "    X_train_processed['country'] = X_train_countries\n",
    "    X_test_processed['country'] = X_test_countries\n",
    "\n",
    "    # --- Modelo 1: SARIMAX (Línea Base Estadística) ---\n",
    "    print(\"Entrenando SARIMAX...\")\n",
    "    sarimax_preds =\n",
    "    y_test_sarimax =\n",
    "    \n",
    "    # Se ajusta un modelo por cada país en el conjunto de prueba\n",
    "    for country in tqdm(X_test_processed['country'].unique(), desc=\"SARIMAX per country\"):\n",
    "        train_country_data = df_featured[df_featured['country'] == country][df_featured['year'].isin(train_years)]\n",
    "        test_country_data = df_featured[df_featured['country'] == country][df_featured['year'].isin(test_years)]\n",
    "        \n",
    "        if not train_country_data.empty and not test_country_data.empty:\n",
    "            endog = train_country_data\n",
    "            exog_cols = [c for c in features if c not in ['year', 'country']]\n",
    "            exog_train = train_country_data[exog_cols]\n",
    "            exog_test = test_country_data[exog_cols]\n",
    "            \n",
    "            try:\n",
    "                # Usamos un orden simple (p,d,q)=(1,1,1) para la demostración\n",
    "                model = SARIMAX(endog, exog=exog_train, order=(1, 1, 1), seasonal_order=(0, 0, 0, 0))\n",
    "                results_sarimax = model.fit(disp=False)\n",
    "                forecast = results_sarimax.get_forecast(steps=len(test_country_data), exog=exog_test)\n",
    "                sarimax_preds.extend(forecast.predicted_mean.values)\n",
    "                y_test_sarimax.extend(test_country_data.values)\n",
    "            except Exception as e:\n",
    "                # Si un modelo falla, predecimos la última observación conocida\n",
    "                sarimax_preds.extend([endog.iloc[-1]] * len(test_country_data))\n",
    "                y_test_sarimax.extend(test_country_data.values)\n",
    "\n",
    "    if y_test_sarimax:\n",
    "        mae = mean_absolute_error(y_test_sarimax, sarimax_preds)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_sarimax, sarimax_preds))\n",
    "        r2 = r2_score(y_test_sarimax, sarimax_preds)\n",
    "        results.append({'MAE': mae, 'RMSE': rmse, 'R2': r2})\n",
    "        print(f\"SARIMAX - MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.2f}\")\n",
    "\n",
    "    # --- Modelos de Machine Learning (usando datos procesados sin 'country') ---\n",
    "    X_train_ml = X_train_processed.drop(columns=['country'])\n",
    "    X_test_ml = X_test_processed.drop(columns=['country'])\n",
    "\n",
    "    # --- Modelo 2: Random Forest ---\n",
    "    print(\"Entrenando Random Forest...\")\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    rf_model.fit(X_train_ml, y_train)\n",
    "    rf_preds = rf_model.predict(X_test_ml)\n",
    "    mae = mean_absolute_error(y_test, rf_preds)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, rf_preds))\n",
    "    r2 = r2_score(y_test, rf_preds)\n",
    "    results.append({'MAE': mae, 'RMSE': rmse, 'R2': r2})\n",
    "    print(f\"Random Forest - MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.2f}\")\n",
    "\n",
    "    # --- Modelo 3: XGBoost ---\n",
    "    print(\"Entrenando XGBoost...\")\n",
    "    xgb_model = xgb.XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    xgb_model.fit(X_train_ml, y_train)\n",
    "    xgb_preds = xgb_model.predict(X_test_ml)\n",
    "    mae = mean_absolute_error(y_test, xgb_preds)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, xgb_preds))\n",
    "    r2 = r2_score(y_test, xgb_preds)\n",
    "    results.append({'MAE': mae, 'RMSE': rmse, 'R2': r2})\n",
    "    print(f\"XGBoost - MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.2f}\")\n",
    "\n",
    "    # --- Modelo 4: LSTM ---\n",
    "    print(\"Entrenando LSTM...\")\n",
    "    \n",
    "    def create_lstm_dataset(X, y, countries, look_back=3):\n",
    "        dataX, dataY =,\n",
    "        for country in countries.unique():\n",
    "            X_country = X[countries == country]\n",
    "            y_country = y[countries == country]\n",
    "            if len(X_country) > look_back:\n",
    "                for i in range(len(X_country) - look_back):\n",
    "                    a = X_country.iloc[i:(i + look_back)].values\n",
    "                    dataX.append(a)\n",
    "                    dataY.append(y_country.iloc[i + look_back])\n",
    "        return np.array(dataX), np.array(dataY)\n",
    "\n",
    "    look_back = 3\n",
    "    X_train_lstm, y_train_lstm = create_lstm_dataset(X_train_ml, y_train, X_train_processed['country'], look_back)\n",
    "    X_test_lstm, y_test_lstm = create_lstm_dataset(X_test_ml, y_test, X_test_processed['country'], look_back)\n",
    "\n",
    "    if X_train_lstm.shape > 0 and X_test_lstm.shape > 0:\n",
    "        # Definir la arquitectura del modelo LSTM\n",
    "        lstm_model = Sequential(, X_train_lstm.shape[2])),\n",
    "            Dropout(0.2),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        lstm_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        \n",
    "        # Entrenar el modelo\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        lstm_model.fit(X_train_lstm, y_train_lstm, epochs=50, batch_size=32, \n",
    "                       validation_data=(X_test_lstm, y_test_lstm), \n",
    "                       callbacks=[early_stopping], verbose=0)\n",
    "        \n",
    "        # Realizar predicciones\n",
    "        lstm_preds = lstm_model.predict(X_test_lstm).flatten()\n",
    "        \n",
    "        mae = mean_absolute_error(y_test_lstm, lstm_preds)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_lstm, lstm_preds))\n",
    "        r2 = r2_score(y_test_lstm, lstm_preds)\n",
    "        results.append({'MAE': mae, 'RMSE': rmse, 'R2': r2})\n",
    "        print(f\"LSTM - MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569c6eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6. ANÁLISIS COMPARATIVO Y SELECCIÓN DEL MODELO\n",
    "# =============================================================================\n",
    "print(\"\\nPaso 5: Analizando y comparando los resultados de los modelos...\")\n",
    "\n",
    "# Convertir los resultados a un DataFrame para fácil visualización\n",
    "summary_list =\n",
    "for model_name, metrics_list in results.items():\n",
    "    if metrics_list:\n",
    "        df_metrics = pd.DataFrame(metrics_list)\n",
    "        summary_list.append({\n",
    "            'Model': model_name,\n",
    "            'MAE_mean': df_metrics['MAE'].mean(),\n",
    "            'MAE_std': df_metrics['MAE'].std(),\n",
    "            'RMSE_mean': df_metrics.mean(),\n",
    "            'RMSE_std': df_metrics.std(),\n",
    "            'R2_mean': df_metrics.mean(),\n",
    "            'R2_std': df_metrics.std()\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_list).set_index('Model')\n",
    "print(\"\\nResumen del rendimiento de los modelos (promedio de los pliegues de CV):\")\n",
    "print(summary_df)\n",
    "\n",
    "# Seleccionar el mejor modelo (basado en el RMSE más bajo)\n",
    "best_model_name = summary_df.idxmin()\n",
    "print(f\"\\nMejor modelo seleccionado: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfae2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7. REENTRENAMIENTO DEL MODELO FINAL Y PRONÓSTICO PARA COLOMBIA\n",
    "# =============================================================================\n",
    "print(f\"\\nPaso 6: Reentrenando el modelo {best_model_name} con todos los datos...\")\n",
    "\n",
    "# Preparar todos los datos para el reentrenamiento\n",
    "X_full_numeric = X.drop(columns=['country'])\n",
    "imputer_final = IterativeImputer(max_iter=10, random_state=42)\n",
    "X_full_imputed = imputer_final.fit_transform(X_full_numeric)\n",
    "scaler_final = StandardScaler()\n",
    "X_full_scaled = scaler_final.fit_transform(X_full_imputed)\n",
    "X_full_processed = pd.DataFrame(X_full_scaled, columns=X_full_numeric.columns, index=X.index)\n",
    "\n",
    "# Reentrenar el modelo XGBoost\n",
    "final_model = xgb.XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "final_model.fit(X_full_processed, y)\n",
    "\n",
    "print(\"Modelo final entrenado. Generando pronósticos para Colombia...\")\n",
    "\n",
    "# --- Creación de escenarios futuros para Colombia ---\n",
    "colombia_last_data = df_featured[df_featured['country'] == 'Colombia'].iloc[-1]\n",
    "future_years = range(2023, 2031)\n",
    "future_df = pd.DataFrame()\n",
    "\n",
    "for year in future_years:\n",
    "    future_row = colombia_last_data.copy()\n",
    "    future_row['year'] = year\n",
    "    future_df = pd.concat(, ignore_index=True)\n",
    "\n",
    "# Escenario A: \"Business as Usual\" (extrapolación simple)\n",
    "for col in:\n",
    "    # Simple extrapolación lineal para la tendencia\n",
    "    last_val = colombia_last_data[col]\n",
    "    trend = (colombia_last_data[col] - df_featured[df_featured['country'] == 'Colombia'][col].iloc[-5:].mean()) / 5\n",
    "    future_df[col] = [last_val + trend * i for i in range(1, len(future_years) + 1)]\n",
    "\n",
    "# Escenario B: \"Inversión Estratégica\" (aumento del 10% en gasto en educación)\n",
    "future_df_optimistic = future_df.copy()\n",
    "future_df_optimistic['gasto_educacion_pib'] *= 1.10\n",
    "\n",
    "# Función para predecir recursivamente\n",
    "def generate_forecast(model, initial_data, future_template, scaler, imputer):\n",
    "    history = initial_data.copy()\n",
    "    predictions =\n",
    "    \n",
    "    for i in range(len(future_template)):\n",
    "        # Preparar la fila para la predicción actual\n",
    "        current_step_features = future_template.iloc[[i]]\n",
    "        \n",
    "        # Actualizar lags y rolling stats con el último dato conocido (de history)\n",
    "        last_known = history.iloc[-1]\n",
    "        for feature in features_to_lag:\n",
    "            current_step_features[f'{feature}_lag_1'] = last_known[feature]\n",
    "            # Para rolling stats, usamos los últimos datos de history\n",
    "            rolling_window = pd.concat()])[feature]\n",
    "            current_step_features[f'{feature}_rolling_mean_3'] = rolling_window.mean()\n",
    "            current_step_features[f'{feature}_rolling_std_3'] = rolling_window.std()\n",
    "\n",
    "        # Preprocesar la fila\n",
    "        current_step_numeric = current_step_features[X_full_numeric.columns]\n",
    "        current_step_imputed = imputer.transform(current_step_numeric)\n",
    "        current_step_scaled = scaler.transform(current_step_imputed)\n",
    "        \n",
    "        # Predecir\n",
    "        prediction = model.predict(current_step_scaled)\n",
    "        predictions.append(prediction)\n",
    "        \n",
    "        # Actualizar 'history' con la nueva predicción para el siguiente paso\n",
    "        new_row = current_step_features.copy()\n",
    "        new_row = prediction\n",
    "        history = pd.concat([history, new_row], ignore_index=True)\n",
    "        \n",
    "    return predictions\n",
    "\n",
    "# Generar pronósticos para ambos escenarios\n",
    "forecast_base = generate_forecast(final_model, df_featured[df_featured['country'] == 'Colombia'], future_df, scaler_final, imputer_final)\n",
    "forecast_optimistic = generate_forecast(final_model, df_featured[df_featured['country'] == 'Colombia'], future_df_optimistic, scaler_final, imputer_final)\n",
    "\n",
    "# --- Visualización de los resultados ---\n",
    "plt.figure(figsize=(18, 9))\n",
    "# Datos históricos\n",
    "plt.plot(df_featured[df_featured['country'] == 'Colombia']['year'], \n",
    "         df_featured[df_featured['country'] == 'Colombia'], \n",
    "         label='Histórico - SiB Colombia', color='black', marker='o')\n",
    "# Pronóstico Base\n",
    "plt.plot(future_years, forecast_base, label='Pronóstico Base (\"Business as Usual\")', color='blue', marker='x', linestyle='--')\n",
    "# Pronóstico Optimista\n",
    "plt.plot(future_years, forecast_optimistic, label='Pronóstico Optimista (↑ Gasto Educación)', color='green', marker='^', linestyle='--')\n",
    "\n",
    "plt.title('Pronóstico de Crecimiento de Publicación de Datos para SiB Colombia (2023-2030)', fontsize=16)\n",
    "plt.xlabel('Año', fontsize=12)\n",
    "plt.ylabel('Número de Registros Publicados (occurrenceCount_publisher)', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nProceso completado.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

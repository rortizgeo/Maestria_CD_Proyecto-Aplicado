{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f75e564d",
   "metadata": {},
   "source": [
    "\n",
    "# Objetivo 2: Generación de modelos\n",
    "\n",
    "Pipeline completo para la predicción del crecimiento de datos de biodiversidad en GBIF.\n",
    "\n",
    "Este script implementa el flujo de trabajo de principio a fin para modelar datos de panel\n",
    "de series temporales, incluyendo:\n",
    "1.  Carga datos de PA_dataAnalysis y preparación\n",
    "2.  Ingeniería de características temporales (lags y ventanas móviles).\n",
    "3.  Un marco de validación cruzada robusto para series de tiempo (ventana expansiva).\n",
    "4.  Preprocesamiento (imputación y escalado) dentro del bucle de validación para evitar fuga de datos.\n",
    "5.  Entrenamiento y evaluación comparativa de cuatro modelos:\n",
    "    - Prophet.\n",
    "    - Random Forest.\n",
    "    - XGBoost.\n",
    "    - Red Neuronal LSTM (para modelado secuencial).\n",
    "6.  Selección del mejor modelo basado en métricas de rendimiento (MAE, RMSE, R²).\n",
    "7.  Reentrenamiento del modelo final y generación de pronósticos para Colombia hasta 2030\n",
    "    bajo dos escenarios de políticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d92da8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. IMPORTACIÓN DE LIBRERÍAS Y CONFIGURACIÓN INICIAL\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Preprocesamiento y modelado de Scikit-Learn\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Modelos especializados\n",
    "\n",
    "import xgboost as xgb\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from prophet import Prophet\n",
    "\n",
    "# Configuraciones generales\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (15, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7bec6131",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Modelado de Deep Learning con TensorFlow/Keras\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LSTM, Dense, Dropout\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages/tensorflow/__init__.py:37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages/tensorflow/python/__init__.py:42\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Bring in subpackages.\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m data\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# from tensorflow.python import keras\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages/tensorflow/python/data/__init__.py:21\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"`tf.data.Dataset` API for input pipelines.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03mSee [Importing Data](https://tensorflow.org/guide/data) for an overview.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AUTOTUNE\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages/tensorflow/python/data/experimental/__init__.py:97\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Experimental API for building input pipelines.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03mThis module contains experimental `Dataset` sources and transformations that can\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m@@UNKNOWN_CARDINALITY\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m service\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatching\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dense_to_ragged_batch\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatching\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dense_to_sparse_batch\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages/tensorflow/python/data/experimental/service/__init__.py:419\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"API for using the tf.data service.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03mThis module contains:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;124;03m  job of ParameterServerStrategy).\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m from_dataset_id\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m register_dataset\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/data_service_ops.py:22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m data_service_pb2\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf2\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compression_ops\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservice\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _pywrap_server_lib\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservice\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _pywrap_utils\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/compression_ops.py:16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Ops for compressing and uncompressing dataset elements.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m structure\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_experimental_dataset_ops \u001b[38;5;28;01mas\u001b[39;00m ged_ops\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompress\u001b[39m(element):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages/tensorflow/python/data/util/structure.py:22\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mitertools\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwrapt\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nest\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m composite_tensor\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ops\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages/tensorflow/python/data/util/nest.py:34\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m\"\"\"## Functions for working with arbitrarily nested sequences of elements.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03mNOTE(mrry): This fork of the `tensorflow.python.util.nest` module\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m   arrays.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sparse_tensor \u001b[38;5;28;01mas\u001b[39;00m _sparse_tensor\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _pywrap_utils\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nest\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages/tensorflow/python/framework/sparse_tensor.py:25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf2\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m composite_tensor\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m constant_op\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dtypes\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ops\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tensor_conversion_registry\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tensor_shape\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tensor_util\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprofiler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m trace\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf_export\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages/tensorflow/python/framework/tensor_util.py:35\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Fallback in case fast_tensor_util is not properly compiled.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fast_tensor_util\n\u001b[1;32m     36\u001b[0m   _FAST_TENSOR_UTIL_AVAILABLE \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32mtensorflow/python/framework/fast_tensor_util.pyx:1\u001b[0m, in \u001b[0;36minit tensorflow.python.framework.fast_tensor_util\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "# Modelado de Deep Learning con TensorFlow/Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24978f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carga de datos\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/rortizgeo/Maestria_CD_Proyecto-Aplicado/main/Data_final.csv\"\n",
    "Data_final = pd.read_csv(url)\n",
    "\n",
    "# Eliminación de columnas por tener muchos vacíos y no ser posible completarlas con imputación. (pensar en otras estrategias)\n",
    "columns_to_drop = ['Overall score', 'areas_protegidas']\n",
    "Data_final = Data_final.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126e13f3",
   "metadata": {},
   "source": [
    "Para aplicar modelos como Random Forest y XGBoost, es necesario agregar características de temporalidad en los datos, para lo cuál es necesario calcular retardos, que se deben aplicar teniendo en cuenta un análisis del ACF Y PACF, así como la incorporación de los tiempos del retardo como hiperparámetros. \n",
    "\n",
    "Los modelos basados en árboles como Random Forest y XGBoost no son conscientes de la secuencia temporal de los datos y no pueden \"extrapolar\" tendencias más allá de los valores que han visto en el entrenamiento. Por lo tanto, es necesario convertir la información temporal en características que el modelo pueda entender. La creación de retardos (lags) y estadísticas de ventana móvil es la técnica estándar para lograrlo. Se podría identificar el número de retardos como un hiperparámetro, guiado por análisis de ACF y PACF (Ver EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cadacbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Paso 2: Realizando ingeniería de características temporales...\n",
      "Ingeniería de características completada.\n",
      "Shape del dataset: (656, 58)\n",
      "Valores NaN restantes: 0\n",
      "\n",
      "Estadísticas de las nuevas características:\n",
      "       occurrenceCount_publisher_lag1  occurrenceCount_publisher_lag2  \\\n",
      "count                    6.560000e+02                    6.560000e+02   \n",
      "mean                     1.530224e+07                    1.257584e+07   \n",
      "std                      5.295431e+07                    4.383457e+07   \n",
      "min                      0.000000e+00                    0.000000e+00   \n",
      "25%                      1.549475e+04                    1.335000e+02   \n",
      "50%                      1.176419e+06                    5.515200e+05   \n",
      "75%                      9.340858e+06                    7.115654e+06   \n",
      "max                      7.401771e+08                    5.946568e+08   \n",
      "\n",
      "       occurrenceCount_publisher_lag3  occurrenceCount_publisher_lag4  \\\n",
      "count                    6.560000e+02                    6.560000e+02   \n",
      "mean                     1.024584e+07                    8.296004e+06   \n",
      "std                      3.656712e+07                    3.057768e+07   \n",
      "min                      0.000000e+00                    0.000000e+00   \n",
      "25%                      0.000000e+00                    0.000000e+00   \n",
      "50%                      2.662950e+05                    1.096710e+05   \n",
      "75%                      5.047951e+06                    3.175013e+06   \n",
      "max                      4.897051e+08                    3.528648e+08   \n",
      "\n",
      "       occurrenceCount_publisher_lag5  occurrenceCount_publisher_rollmean3  \\\n",
      "count                    6.560000e+02                         6.560000e+02   \n",
      "mean                     6.738803e+06                         1.668685e+07   \n",
      "std                      2.673301e+07                         5.368850e+07   \n",
      "min                      0.000000e+00                         0.000000e+00   \n",
      "25%                      0.000000e+00                         7.333842e+04   \n",
      "50%                      1.874800e+04                         1.624914e+06   \n",
      "75%                      2.448169e+06                         1.190595e+07   \n",
      "max                      3.442562e+08                         6.674169e+08   \n",
      "\n",
      "       occurrenceCount_publisher_rollstd3  \\\n",
      "count                        6.560000e+02   \n",
      "mean                         4.812063e+06   \n",
      "std                          2.360976e+07   \n",
      "min                          0.000000e+00   \n",
      "25%                          8.972601e+03   \n",
      "50%                          3.353990e+05   \n",
      "75%                          2.509022e+06   \n",
      "max                          5.233843e+08   \n",
      "\n",
      "       occurrenceCount_publisher_rollmean5  \\\n",
      "count                         6.560000e+02   \n",
      "mean                          1.657737e+07   \n",
      "std                           4.957963e+07   \n",
      "min                           0.000000e+00   \n",
      "25%                           1.419378e+05   \n",
      "50%                           2.011318e+06   \n",
      "75%                           1.261729e+07   \n",
      "max                           5.443510e+08   \n",
      "\n",
      "       occurrenceCount_publisher_rollstd5  \\\n",
      "count                        6.560000e+02   \n",
      "mean                         7.874283e+06   \n",
      "std                          2.826472e+07   \n",
      "min                          0.000000e+00   \n",
      "25%                          4.817769e+04   \n",
      "50%                          1.016754e+06   \n",
      "75%                          6.251451e+06   \n",
      "max                          3.898861e+08   \n",
      "\n",
      "       occurrenceCount_publisher_rollmean7  ...  gasto_educacion_pib_lag2  \\\n",
      "count                         6.560000e+02  ...                656.000000   \n",
      "mean                          1.649014e+07  ...                  4.266209   \n",
      "std                           4.648245e+07  ...                  2.122214   \n",
      "min                           0.000000e+00  ...                  0.000000   \n",
      "25%                           2.050516e+05  ...                  3.148687   \n",
      "50%                           2.304801e+06  ...                  4.781910   \n",
      "75%                           1.425623e+07  ...                  5.510889   \n",
      "max                           4.764592e+08  ...                  8.583830   \n",
      "\n",
      "       gasto_educacion_pib_lag3  gasto_educacion_pib_lag4  \\\n",
      "count                656.000000                656.000000   \n",
      "mean                   3.949542                  3.644751   \n",
      "std                    2.314343                  2.458819   \n",
      "min                    0.000000                  0.000000   \n",
      "25%                    2.549517                  0.830940   \n",
      "50%                    4.632265                  4.480175   \n",
      "75%                    5.462904                  5.431867   \n",
      "max                    8.559550                  8.559550   \n",
      "\n",
      "       gasto_educacion_pib_lag5  gasto_educacion_pib_rollmean3  \\\n",
      "count                656.000000                     656.000000   \n",
      "mean                   3.342840                       4.855040   \n",
      "std                    2.563963                       1.447663   \n",
      "min                    0.000000                       0.000000   \n",
      "25%                    0.000000                       4.100878   \n",
      "50%                    4.224074                       4.932528   \n",
      "75%                    5.389558                       5.550119   \n",
      "max                    8.559550                       8.497764   \n",
      "\n",
      "       gasto_educacion_pib_rollstd3  gasto_educacion_pib_rollmean5  \\\n",
      "count                    656.000000                     656.000000   \n",
      "mean                       0.277200                       4.856486   \n",
      "std                        0.361797                       1.407998   \n",
      "min                        0.000000                       0.000000   \n",
      "25%                        0.088735                       4.122125   \n",
      "50%                        0.175620                       4.932367   \n",
      "75%                        0.331003                       5.533535   \n",
      "max                        3.467356                       8.245108   \n",
      "\n",
      "       gasto_educacion_pib_rollstd5  gasto_educacion_pib_rollmean7  \\\n",
      "count                    656.000000                     656.000000   \n",
      "mean                       0.391173                       4.856149   \n",
      "std                        0.402508                       1.373718   \n",
      "min                        0.000000                       0.000000   \n",
      "25%                        0.153728                       4.130995   \n",
      "50%                        0.278714                       4.933646   \n",
      "75%                        0.480313                       5.545625   \n",
      "max                        2.898983                       8.048063   \n",
      "\n",
      "       gasto_educacion_pib_rollstd7  \n",
      "count                    656.000000  \n",
      "mean                       0.476935  \n",
      "std                        0.425371  \n",
      "min                        0.000000  \n",
      "25%                        0.228329  \n",
      "50%                        0.358580  \n",
      "75%                        0.563539  \n",
      "max                        2.716642  \n",
      "\n",
      "[8 rows x 44 columns]\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 2. INGENIERÍA DE CARACTERÍSTICAS TEMPORALES (OPTIMIZADA)\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nPaso 2: Realizando ingeniería de características temporales...\")\n",
    "\n",
    "TARGET = 'occurrenceCount_publisher'\n",
    "\n",
    "def create_temporal_features_optimized(data, features_to_lag, \n",
    "                                     lags=[1, 2, 3, 4, 5], \n",
    "                                     roll_windows=[3, 5, 7],\n",
    "                                     fill_na=0):\n",
    "    \"\"\"\n",
    "    Genera características temporales y completa automáticamente con 0\n",
    "    los valores NaN generados, según la lógica del negocio.\n",
    "    \n",
    "    Parámetros\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        Dataset con al menos 'country' y variables numéricas.\n",
    "    features_to_lag : list\n",
    "        Lista de columnas numéricas a transformar.\n",
    "    lags : list\n",
    "        Lista de retardos.\n",
    "    roll_windows : list\n",
    "        Lista de ventanas móviles.\n",
    "    fill_na : int/float\n",
    "        Valor para completar NaN (0 por defecto según lógica de negocio).\n",
    "        \n",
    "    Retorna\n",
    "    -------\n",
    "    DataFrame con nuevas características y NaN completados.\n",
    "    \"\"\"\n",
    "    \n",
    "    df_copy = data.copy()\n",
    "    \n",
    "    for feature in features_to_lag:\n",
    "        # Características de lag\n",
    "        for lag in lags:\n",
    "            lag_col = f'{feature}_lag{lag}'\n",
    "            df_copy[lag_col] = df_copy.groupby('country')[feature].shift(lag)\n",
    "            df_copy[lag_col] = df_copy[lag_col].fillna(fill_na)\n",
    "        \n",
    "        # Características de ventana móvil\n",
    "        for w in roll_windows:\n",
    "            # Rolling mean\n",
    "            mean_col = f'{feature}_rollmean{w}'\n",
    "            df_copy[mean_col] = (\n",
    "                df_copy.groupby('country')[feature]\n",
    "                .shift(1)\n",
    "                .rolling(window=w, min_periods=1)\n",
    "                .mean()\n",
    "            )\n",
    "            df_copy[mean_col] = df_copy[mean_col].fillna(fill_na)\n",
    "            \n",
    "            # Rolling std\n",
    "            std_col = f'{feature}_rollstd{w}'\n",
    "            df_copy[std_col] = (\n",
    "                df_copy.groupby('country')[feature]\n",
    "                .shift(1)\n",
    "                .rolling(window=w, min_periods=1)\n",
    "                .std()\n",
    "            )\n",
    "            df_copy[std_col] = df_copy[std_col].fillna(fill_na)\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# ===========================\n",
    "# Uso del código optimizado\n",
    "# ===========================\n",
    "features_to_lag = [\n",
    "    \"occurrenceCount_publisher\", \"pib_per_capita\",\n",
    "    \"gasto_educacion_gobierno\", \"gasto_educacion_pib\"\n",
    "]\n",
    "\n",
    "# Crear dataset con nuevas features (completando con 0)\n",
    "df_featured = create_temporal_features_optimized(\n",
    "    Data_final,\n",
    "    features_to_lag=features_to_lag,\n",
    "    lags=[1, 2, 3, 4, 5],  # Reducido para evitar overfitting\n",
    "    roll_windows=[3, 5, 7],  # Reducido para evitar overfitting\n",
    "    fill_na=0  # ¡IMPORTANTE! Completar con 0 según lógica de negocio\n",
    ")\n",
    "\n",
    "print(\"Ingeniería de características completada.\")\n",
    "print(f\"Shape del dataset: {df_featured.shape}\")\n",
    "print(f\"Valores NaN restantes: {df_featured.isnull().sum().sum()}\")\n",
    "\n",
    "# Mostrar estadísticas de las nuevas características\n",
    "print(\"\\nEstadísticas de las nuevas características:\")\n",
    "new_features = [col for col in df_featured.columns if any(x in col for x in ['_lag', '_roll'])]\n",
    "print(df_featured[new_features].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed0b8e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución de valores en características temporales:\n",
      "occurrenceCount_publisher_lag1: 18.8% ceros\n",
      "occurrenceCount_publisher_lag2: 25.0% ceros\n",
      "occurrenceCount_publisher_lag3: 31.2% ceros\n",
      "occurrenceCount_publisher_lag4: 37.5% ceros\n",
      "occurrenceCount_publisher_lag5: 43.8% ceros\n",
      "occurrenceCount_publisher_rollmean3: 9.9% ceros\n",
      "occurrenceCount_publisher_rollstd3: 15.4% ceros\n",
      "occurrenceCount_publisher_rollmean5: 5.9% ceros\n",
      "occurrenceCount_publisher_rollstd5: 7.6% ceros\n",
      "occurrenceCount_publisher_rollmean7: 3.7% ceros\n",
      "occurrenceCount_publisher_rollstd7: 4.3% ceros\n",
      "pib_per_capita_lag1: 6.2% ceros\n",
      "pib_per_capita_lag2: 12.5% ceros\n",
      "pib_per_capita_lag3: 18.8% ceros\n",
      "pib_per_capita_lag4: 25.0% ceros\n",
      "pib_per_capita_lag5: 31.2% ceros\n",
      "pib_per_capita_rollmean3: 0.2% ceros\n",
      "pib_per_capita_rollstd3: 0.3% ceros\n",
      "pib_per_capita_rollmean5: 0.2% ceros\n",
      "pib_per_capita_rollstd5: 0.3% ceros\n",
      "pib_per_capita_rollmean7: 0.2% ceros\n",
      "pib_per_capita_rollstd7: 0.3% ceros\n",
      "gasto_educacion_gobierno_lag1: 6.2% ceros\n",
      "gasto_educacion_gobierno_lag2: 12.5% ceros\n",
      "gasto_educacion_gobierno_lag3: 18.8% ceros\n",
      "gasto_educacion_gobierno_lag4: 25.0% ceros\n",
      "gasto_educacion_gobierno_lag5: 31.2% ceros\n",
      "gasto_educacion_gobierno_rollmean3: 0.2% ceros\n",
      "gasto_educacion_gobierno_rollstd3: 0.3% ceros\n",
      "gasto_educacion_gobierno_rollmean5: 0.2% ceros\n",
      "gasto_educacion_gobierno_rollstd5: 0.3% ceros\n",
      "gasto_educacion_gobierno_rollmean7: 0.2% ceros\n",
      "gasto_educacion_gobierno_rollstd7: 0.3% ceros\n",
      "gasto_educacion_pib_lag1: 6.2% ceros\n",
      "gasto_educacion_pib_lag2: 12.5% ceros\n",
      "gasto_educacion_pib_lag3: 18.8% ceros\n",
      "gasto_educacion_pib_lag4: 25.0% ceros\n",
      "gasto_educacion_pib_lag5: 31.2% ceros\n",
      "gasto_educacion_pib_rollmean3: 0.2% ceros\n",
      "gasto_educacion_pib_rollstd3: 0.3% ceros\n",
      "gasto_educacion_pib_rollmean5: 0.2% ceros\n",
      "gasto_educacion_pib_rollstd5: 0.3% ceros\n",
      "gasto_educacion_pib_rollmean7: 0.2% ceros\n",
      "gasto_educacion_pib_rollstd7: 0.3% ceros\n"
     ]
    }
   ],
   "source": [
    "# Validar que no quedan NaN\n",
    "assert df_featured.isnull().sum().sum() == 0, \"¡Aún hay valores NaN!\"\n",
    "\n",
    "# Verificar distribución de las nuevas features\n",
    "print(\"Distribución de valores en características temporales:\")\n",
    "for col in new_features:\n",
    "    zero_percentage = (df_featured[col] == 0).mean() * 100\n",
    "    print(f\"{col}: {zero_percentage:.1f}% ceros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1dfbcc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Paso 3: Preparando el marco de validación y los datos para el modelado...\n",
      "Features seleccionadas: ['PC1', 'PC2', 'pib_per_capita', 'gasto_educacion_gobierno', 'gasto_educacion_pib', 'superficie_total_km2', 'country', 'region', 'occurrenceCount_publisher_lag2', 'occurrenceCount_publisher_rollmean3', 'occurrenceCount_publisher_rollstd3']\n",
      "X shape: (656, 11)\n",
      "y shape: (656,)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 3. PREPARACIÓN PARA EL MODELADO Y VALIDACIÓN\n",
    "# =============================================================================\n",
    "print(\"\\nPaso 3: Preparando el marco de validación y los datos para el modelado...\")\n",
    "\n",
    "# Definir variable objetivo\n",
    "TARGET = 'occurrenceCount_publisher'\n",
    "\n",
    "# Definir variables predictoras (Decidir con Daniel cuáles usar basándose en EDA y disponibilidad. Preguntar si es posible usar todas sin complejizar el modelo)\n",
    "features = [\n",
    "    'PC1', 'PC2', 'pib_per_capita', 'gasto_educacion_gobierno',\n",
    "    'gasto_educacion_pib', 'superficie_total_km2', \"country\", \"region\", \"incomeLevel\"\n",
    "    f\"{TARGET}_lag1\", f\"{TARGET}_lag2\", f\"{TARGET}_rollmean3\", f\"{TARGET}_rollstd3\"\n",
    "]\n",
    "\n",
    "# Filtrar las variables que realmente existen en el DataFrame\n",
    "features = [f for f in features if f in df_featured.columns]\n",
    "\n",
    "# Definir X (predictoras) e y (objetivo)\n",
    "X = df_featured[features].copy()\n",
    "y = df_featured[TARGET].copy()\n",
    "\n",
    "# Configurar validación cruzada para series de tiempo\n",
    "n_splits = 5  # número de pliegues\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Diccionario para almacenar resultados\n",
    "results = {\n",
    "    'Prophet': None,\n",
    "    'RandomForest': None,\n",
    "    'XGBoost': None,\n",
    "    'LSTM': None\n",
    "}\n",
    "\n",
    "print(\"Features seleccionadas:\", features)\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd4ed319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Paso 4: Iniciando el bucle de entrenamiento y validación de modelos...\n",
      "\n",
      "===== FOLD 2/5 =====\n",
      "Años de entrenamiento: 2007 - 2012\n",
      "Años de prueba: 2013 - 2014\n",
      "Forma de X_train_processed: (246, 10)\n",
      "Forma de X_test_processed: (82, 10)\n",
      "\n",
      "===== FOLD 3/5 =====\n",
      "Años de entrenamiento: 2007 - 2014\n",
      "Años de prueba: 2015 - 2016\n",
      "Forma de X_train_processed: (328, 10)\n",
      "Forma de X_test_processed: (82, 10)\n",
      "\n",
      "===== FOLD 4/5 =====\n",
      "Años de entrenamiento: 2007 - 2016\n",
      "Años de prueba: 2017 - 2018\n",
      "Forma de X_train_processed: (410, 10)\n",
      "Forma de X_test_processed: (82, 10)\n",
      "\n",
      "===== FOLD 5/5 =====\n",
      "Años de entrenamiento: 2007 - 2018\n",
      "Años de prueba: 2019 - 2020\n",
      "Forma de X_train_processed: (492, 10)\n",
      "Forma de X_test_processed: (82, 10)\n",
      "\n",
      "===== FOLD 6/5 =====\n",
      "Años de entrenamiento: 2007 - 2020\n",
      "Años de prueba: 2021 - 2022\n",
      "Forma de X_train_processed: (574, 10)\n",
      "Forma de X_test_processed: (82, 10)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 4. BUCLE DE ENTRENAMIENTO \n",
    "# =============================================================================\n",
    "print(\"\\nPaso 4: Iniciando el bucle de entrenamiento y validación de modelos...\")\n",
    "\n",
    "# Inicializar resultados correctamente\n",
    "results = {\n",
    "    'Prophet': [],\n",
    "    'RandomForest': [],\n",
    "    'XGBoost': [],\n",
    "    'LSTM': []\n",
    "}\n",
    "\n",
    "unique_years = df_featured['year'].unique()\n",
    "unique_years.sort()\n",
    "\n",
    "for fold, (train_year_idx, test_year_idx) in enumerate(tscv.split(unique_years)):\n",
    "    print(f\"\\n===== FOLD {fold + 2}/{n_splits} =====\")\n",
    "    \n",
    "    # Identificar los años de entrenamiento y prueba\n",
    "    train_years = [unique_years[i] for i in train_year_idx]\n",
    "    test_years = [unique_years[i] for i in test_year_idx]\n",
    "    print(f\"Años de entrenamiento: {min(train_years)} - {max(train_years)}\")\n",
    "    print(f\"Años de prueba: {min(test_years)} - {max(test_years)}\")\n",
    "\n",
    "    # Dividir los datos en entrenamiento y prueba según los años\n",
    "    train_indices = df_featured[df_featured['year'].isin(train_years)].index\n",
    "    test_indices = df_featured[df_featured['year'].isin(test_years)].index\n",
    "\n",
    "    X_train, X_test = X.loc[train_indices], X.loc[test_indices]\n",
    "    y_train, y_test = y.loc[train_indices], y.loc[test_indices]\n",
    "\n",
    "    # --- Preprocesamiento DENTRO del bucle ---\n",
    "    # Guardar información de países antes de procesar\n",
    "    X_train_countries = X_train['country'] if 'country' in X_train.columns else None\n",
    "    X_test_countries = X_test['country'] if 'country' in X_test.columns else None\n",
    "    \n",
    "    # Seleccionar solo características numéricas y eliminar columnas con todos NaN\n",
    "    numeric_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "    \n",
    "    # Filtrar columnas que no son completamente NaN\n",
    "    valid_numeric_features = []\n",
    "    for col in numeric_features:\n",
    "        if not X_train[col].isnull().all():  # Solo columnas con al menos algún valor\n",
    "            valid_numeric_features.append(col)\n",
    "    \n",
    "    X_train_numeric = X_train[valid_numeric_features]\n",
    "    X_test_numeric = X_test[valid_numeric_features]\n",
    "\n",
    "    # Imputación de valores faltantes usando IterativeImputer (Puede no ser necesario ya que los datos se limpiaron y se elimnarcon columnas con NaN, incluso enla ingeniería de características)\n",
    "    imputer = IterativeImputer(max_iter=10, random_state=42)\n",
    "    X_train_imputed = imputer.fit_transform(X_train_numeric)\n",
    "    X_test_imputed = imputer.transform(X_test_numeric)\n",
    "\n",
    "    # Escalado de características\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "    X_test_scaled = scaler.transform(X_test_imputed)\n",
    "    \n",
    "    # Reconstruir DataFrames con las columnas válidas\n",
    "    X_train_processed = pd.DataFrame(X_train_scaled, columns=valid_numeric_features, index=X_train.index)\n",
    "    X_test_processed = pd.DataFrame(X_test_scaled, columns=valid_numeric_features, index=X_test.index)\n",
    "    \n",
    "    # Restaurar información de países si existe\n",
    "    if X_train_countries is not None:\n",
    "        X_train_processed['country'] = X_train_countries\n",
    "    if X_test_countries is not None:\n",
    "        X_test_processed['country'] = X_test_countries\n",
    "\n",
    "    print(f\"Forma de X_train_processed: {X_train_processed.shape}\")\n",
    "    print(f\"Forma de X_test_processed: {X_test_processed.shape}\")\n",
    "\n",
    "    # CONTINUAR CON LOS MODELOS..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16312afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando Prophet...\n",
      "Error con Prophet para Andorra: Unable to parse string \"Europe & Central Asia\" at position 0\n",
      "Error con Prophet para Argentina: Unable to parse string \"Latin America & Caribbean \" at position 0\n",
      "Error con Prophet para Australia: Unable to parse string \"East Asia & Pacific\" at position 0\n",
      "Error con Prophet para Belgium: Unable to parse string \"Europe & Central Asia\" at position 0\n",
      "Error con Prophet para Benin: Unable to parse string \"Sub-Saharan Africa \" at position 0\n",
      "Error con Prophet para Brazil: Unable to parse string \"Latin America & Caribbean \" at position 0\n",
      "Error con Prophet para Canada: Unable to parse string \"North America\" at position 0\n",
      "Error con Prophet para Central African Republic: Unable to parse string \"Sub-Saharan Africa \" at position 0\n",
      "Error con Prophet para Chile: Unable to parse string \"Latin America & Caribbean \" at position 0\n",
      "Error con Prophet para Colombia: Unable to parse string \"Latin America & Caribbean \" at position 0\n",
      "Error con Prophet para Costa Rica: Unable to parse string \"Latin America & Caribbean \" at position 0\n",
      "Error con Prophet para Denmark: Unable to parse string \"Europe & Central Asia\" at position 0\n",
      "Error con Prophet para Estonia: Unable to parse string \"Europe & Central Asia\" at position 0\n",
      "Error con Prophet para Finland: Unable to parse string \"Europe & Central Asia\" at position 0\n",
      "Error con Prophet para France: Unable to parse string \"Europe & Central Asia\" at position 0\n",
      "Error con Prophet para Germany: Unable to parse string \"Europe & Central Asia\" at position 0\n",
      "Error con Prophet para Guinea: Unable to parse string \"Sub-Saharan Africa \" at position 0\n",
      "Error con Prophet para Iceland: Unable to parse string \"Europe & Central Asia\" at position 0\n",
      "Error con Prophet para Ireland: Unable to parse string \"Europe & Central Asia\" at position 0\n",
      "Error con Prophet para Luxembourg: Unable to parse string \"Europe & Central Asia\" at position 0\n",
      "Error con Prophet para Madagascar: Unable to parse string \"Sub-Saharan Africa \" at position 0\n",
      "Error con Prophet para Mauritania: Unable to parse string \"Sub-Saharan Africa \" at position 0\n",
      "Error con Prophet para Mexico: Unable to parse string \"Latin America & Caribbean \" at position 0\n",
      "Error con Prophet para Mongolia: Unable to parse string \"East Asia & Pacific\" at position 0\n",
      "Error con Prophet para Netherlands: Unable to parse string \"Europe & Central Asia\" at position 0\n",
      "Error con Prophet para New Zealand: Unable to parse string \"East Asia & Pacific\" at position 0\n",
      "Error con Prophet para Norway: Unable to parse string \"Europe & Central Asia\" at position 0\n",
      "Error con Prophet para Peru: Unable to parse string \"Latin America & Caribbean \" at position 0\n",
      "Error con Prophet para Poland: Unable to parse string \"Europe & Central Asia\" at position 0\n",
      "Error con Prophet para Portugal: Unable to parse string \"Europe & Central Asia\" at position 0\n",
      "Error con Prophet para Slovak Republic: Unable to parse string \"Europe & Central Asia\" at position 0\n",
      "Error con Prophet para Slovenia: Unable to parse string \"Europe & Central Asia\" at position 0\n",
      "Error con Prophet para South Africa: Unable to parse string \"Sub-Saharan Africa \" at position 0\n",
      "Error con Prophet para Spain: Unable to parse string \"Europe & Central Asia\" at position 0\n",
      "Error con Prophet para Suriname: Unable to parse string \"Latin America & Caribbean \" at position 0\n",
      "Error con Prophet para Sweden: Unable to parse string \"Europe & Central Asia\" at position 0\n",
      "Error con Prophet para Switzerland: Unable to parse string \"Europe & Central Asia\" at position 0\n",
      "Error con Prophet para Togo: Unable to parse string \"Sub-Saharan Africa \" at position 0\n",
      "Error con Prophet para United Kingdom: Unable to parse string \"Europe & Central Asia\" at position 0\n",
      "Error con Prophet para United States: Unable to parse string \"North America\" at position 0\n",
      "Error con Prophet para Uruguay: Unable to parse string \"Latin America & Caribbean \" at position 0\n",
      "Prophet - MAE: 10,531,492.06, RMSE: 36,096,328.50, R2: 0.92\n"
     ]
    }
   ],
   "source": [
    "# --- Modelo 1: Prophet (Línea Base Moderna) ---\n",
    "print(\"Entrenando Prophet...\")\n",
    "prophet_preds = []\n",
    "y_test_prophet = []\n",
    "    \n",
    "regressor_cols = [col for col in features if col not in ['year', 'country'] and col in df_featured.columns]\n",
    "\n",
    "for country in X_test_processed['country'].unique():\n",
    "        train_country_df = df_featured[(df_featured['country'] == country) & (df_featured['year'].isin(train_years))].copy()\n",
    "        test_country_df = df_featured[(df_featured['country'] == country) & (df_featured['year'].isin(test_years))].copy()\n",
    "        \n",
    "        if not train_country_df.empty and not test_country_df.empty:\n",
    "            # Preparar datos para Prophet (ds, y)\n",
    "            train_country_df['ds'] = pd.to_datetime(train_country_df['year'].astype(str), format='%Y')\n",
    "            train_country_df.rename(columns={TARGET: 'y'}, inplace=True)\n",
    "            \n",
    "            # Instanciar y entrenar el modelo\n",
    "            model = Prophet()\n",
    "            for regressor in regressor_cols:\n",
    "                if regressor in train_country_df.columns:\n",
    "                    model.add_regressor(regressor)\n",
    "            \n",
    "            try:\n",
    "                model.fit(train_country_df[['ds', 'y'] + regressor_cols])\n",
    "                \n",
    "                # Preparar dataframe futuro para predicción\n",
    "                future_df = test_country_df.copy()\n",
    "                future_df['ds'] = pd.to_datetime(future_df['year'].astype(str), format='%Y')\n",
    "                \n",
    "                # Predecir\n",
    "                forecast = model.predict(future_df[['ds'] + regressor_cols])\n",
    "                prophet_preds.extend(forecast['yhat'].values)\n",
    "                y_test_prophet.extend(test_country_df[TARGET].values)\n",
    "            except Exception as e:\n",
    "                print(f\"Error con Prophet para {country}: {e}\")\n",
    "                # Si el modelo falla, predecir la última observación conocida\n",
    "                last_known_value = train_country_df['y'].iloc[-1] if not train_country_df.empty else 0\n",
    "                prophet_preds.extend([last_known_value] * len(test_country_df))\n",
    "                y_test_prophet.extend(test_country_df[TARGET].values)\n",
    "\n",
    "if y_test_prophet and prophet_preds:\n",
    "        mae = mean_absolute_error(y_test_prophet, prophet_preds)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_prophet, prophet_preds))\n",
    "        r2 = r2_score(y_test_prophet, prophet_preds)\n",
    "        results['Prophet'].append({'MAE': mae, 'RMSE': rmse, 'R2': r2})\n",
    "        print(f\"Prophet - MAE: {mae:,.2f}, RMSE: {rmse:,.2f}, R2: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "913e2908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando Random Forest...\n",
      "Random Forest - MAE: 11,198,823.88, RMSE: 49,677,571.03, R2: 0.85\n"
     ]
    }
   ],
   "source": [
    "# --- Modelos de Machine Learning ---\n",
    "    # Preparar datos para modelos ML (sin la columna 'country')\n",
    "X_train_ml = X_train_processed.drop(columns=['country']) if 'country' in X_train_processed.columns else X_train_processed\n",
    "X_test_ml = X_test_processed.drop(columns=['country']) if 'country' in X_test_processed.columns else X_test_processed\n",
    "y_train_ml = y_train\n",
    "y_test_ml = y_test\n",
    "\n",
    "# --- Modelo 2: Random Forest ---\n",
    "print(\"Entrenando Random Forest...\")\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train_ml, y_train_ml)\n",
    "rf_preds = rf_model.predict(X_test_ml)\n",
    "mae = mean_absolute_error(y_test_ml, rf_preds)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_ml, rf_preds))\n",
    "r2 = r2_score(y_test_ml, rf_preds)\n",
    "results['RandomForest'].append({'MAE': mae, 'RMSE': rmse, 'R2': r2})\n",
    "print(f\"Random Forest - MAE: {mae:,.2f}, RMSE: {rmse:,.2f}, R2: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf735855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando XGBoost...\n",
      "XGBoost - MAE: 9,453,252.70, RMSE: 40,720,390.00, R2: 0.90\n"
     ]
    }
   ],
   "source": [
    "   # --- Modelo 3: XGBoost ---\n",
    "print(\"Entrenando XGBoost...\")\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "xgb_model.fit(X_train_ml, y_train_ml)\n",
    "xgb_preds = xgb_model.predict(X_test_ml)\n",
    "mae = mean_absolute_error(y_test_ml, xgb_preds)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_ml, xgb_preds))\n",
    "r2 = r2_score(y_test_ml, xgb_preds)\n",
    "results['XGBoost'].append({'MAE': mae, 'RMSE': rmse, 'R2': r2})\n",
    "print(f\"XGBoost - MAE: {mae:,.2f}, RMSE: {rmse:,.2f}, R2: {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b378069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando LSTM...\n",
      "Secuencias creadas: 451\n",
      "Secuencias creadas: 0\n",
      "Secuencias de entrenamiento: (451, 3, 9)\n",
      "Secuencias de prueba: (0,)\n",
      "LSTM: No hay suficientes datos para este fold\n"
     ]
    }
   ],
   "source": [
    "# --- Modelo 4: LSTM ---\n",
    "\n",
    "# --- Modelo 4: LSTM (VERSIÓN CORREGIDA) ---\n",
    "print(\"Entrenando LSTM...\")\n",
    "\n",
    "def create_lstm_dataset_corrected(X, y, countries, look_back=3):\n",
    "    \"\"\"\n",
    "    Versión corregida: X debe incluir los datos, countries es la serie separada\n",
    "    \"\"\"\n",
    "    dataX, dataY = [],[]\n",
    "    \n",
    "    # Verificar que tenemos el mismo número de muestras en X y countries\n",
    "    assert len(X) == len(countries), \"X y countries deben tener la misma longitud\"\n",
    "    \n",
    "    for country in countries.unique():\n",
    "        # Obtener índices para este país\n",
    "        country_indices = countries[countries == country].index\n",
    "        \n",
    "        # Filtrar X y y usando los índices\n",
    "        X_country = X.loc[country_indices]\n",
    "        y_country = y.loc[country_indices]\n",
    "        \n",
    "        if len(X_country) > look_back:\n",
    "            for i in range(len(X_country) - look_back):\n",
    "                a = X_country.iloc[i:(i + look_back)].values\n",
    "                dataX.append(a)\n",
    "                dataY.append(y_country.iloc[i + look_back])\n",
    "    \n",
    "    print(f\"Secuencias creadas: {len(dataX)}\")\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "look_back = 3\n",
    "\n",
    "# ✅ USAR X_train_ml (sin 'country') pero pasar X_train_processed['country'] separado\n",
    "X_train_lstm, y_train_lstm = create_lstm_dataset_corrected(\n",
    "    X_train_ml,  # Datos numéricos sin 'country'\n",
    "    y_train_ml,   # Target\n",
    "    X_train_processed['country'],  # Columna 'country' separada\n",
    "    look_back\n",
    ")\n",
    "\n",
    "X_test_lstm, y_test_lstm = create_lstm_dataset_corrected(\n",
    "    X_test_ml, \n",
    "    y_test_ml, \n",
    "    X_test_processed['country'], \n",
    "    look_back\n",
    ")\n",
    "\n",
    "print(f\"Secuencias de entrenamiento: {X_train_lstm.shape}\")\n",
    "print(f\"Secuencias de prueba: {X_test_lstm.shape}\")\n",
    "\n",
    "if X_train_lstm.shape[0] > 0 and X_test_lstm.shape[0] > 0:\n",
    "    # ... resto del código LSTM\n",
    "    pass  # Aquí iría el entrenamiento y predicción del modelo LSTM\n",
    "else:\n",
    "    print(\"LSTM: No hay suficientes datos para este fold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569c6eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6. ANÁLISIS COMPARATIVO Y SELECCIÓN DEL MODELO\n",
    "# =============================================================================\n",
    "print(\"\\nPaso 5: Analizando y comparando los resultados de los modelos...\")\n",
    "\n",
    "# Convertir los resultados a un DataFrame para fácil visualización\n",
    "summary_list =\n",
    "for model_name, metrics_list in results.items():\n",
    "    if metrics_list:\n",
    "        df_metrics = pd.DataFrame(metrics_list)\n",
    "        summary_list.append({\n",
    "            'Model': model_name,\n",
    "            'MAE_mean': df_metrics['MAE'].mean(),\n",
    "            'MAE_std': df_metrics['MAE'].std(),\n",
    "            'RMSE_mean': df_metrics.mean(),\n",
    "            'RMSE_std': df_metrics.std(),\n",
    "            'R2_mean': df_metrics.mean(),\n",
    "            'R2_std': df_metrics.std()\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_list).set_index('Model')\n",
    "print(\"\\nResumen del rendimiento de los modelos (promedio de los pliegues de CV):\")\n",
    "print(summary_df)\n",
    "\n",
    "# Seleccionar el mejor modelo (basado en el RMSE más bajo)\n",
    "best_model_name = summary_df.idxmin()\n",
    "print(f\"\\nMejor modelo seleccionado: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfae2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7. REENTRENAMIENTO DEL MODELO FINAL Y PRONÓSTICO PARA COLOMBIA\n",
    "# =============================================================================\n",
    "print(f\"\\nPaso 6: Reentrenando el modelo {best_model_name} con todos los datos...\")\n",
    "\n",
    "# Preparar todos los datos para el reentrenamiento\n",
    "X_full_numeric = X.drop(columns=['country'])\n",
    "imputer_final = IterativeImputer(max_iter=10, random_state=42)\n",
    "X_full_imputed = imputer_final.fit_transform(X_full_numeric)\n",
    "scaler_final = StandardScaler()\n",
    "X_full_scaled = scaler_final.fit_transform(X_full_imputed)\n",
    "X_full_processed = pd.DataFrame(X_full_scaled, columns=X_full_numeric.columns, index=X.index)\n",
    "\n",
    "# Reentrenar el modelo XGBoost\n",
    "final_model = xgb.XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "final_model.fit(X_full_processed, y)\n",
    "\n",
    "print(\"Modelo final entrenado. Generando pronósticos para Colombia...\")\n",
    "\n",
    "# --- Creación de escenarios futuros para Colombia ---\n",
    "colombia_last_data = df_featured[df_featured['country'] == 'Colombia'].iloc[-1]\n",
    "future_years = range(2023, 2031)\n",
    "future_df = pd.DataFrame()\n",
    "\n",
    "for year in future_years:\n",
    "    future_row = colombia_last_data.copy()\n",
    "    future_row['year'] = year\n",
    "    future_df = pd.concat(, ignore_index=True)\n",
    "\n",
    "# Escenario A: \"Business as Usual\" (extrapolación simple)\n",
    "for col in:\n",
    "    # Simple extrapolación lineal para la tendencia\n",
    "    last_val = colombia_last_data[col]\n",
    "    trend = (colombia_last_data[col] - df_featured[df_featured['country'] == 'Colombia'][col].iloc[-5:].mean()) / 5\n",
    "    future_df[col] = [last_val + trend * i for i in range(1, len(future_years) + 1)]\n",
    "\n",
    "# Escenario B: \"Inversión Estratégica\" (aumento del 10% en gasto en educación)\n",
    "future_df_optimistic = future_df.copy()\n",
    "future_df_optimistic['gasto_educacion_pib'] *= 1.10\n",
    "\n",
    "# Función para predecir recursivamente\n",
    "def generate_forecast(model, initial_data, future_template, scaler, imputer):\n",
    "    history = initial_data.copy()\n",
    "    predictions =\n",
    "    \n",
    "    for i in range(len(future_template)):\n",
    "        # Preparar la fila para la predicción actual\n",
    "        current_step_features = future_template.iloc[[i]]\n",
    "        \n",
    "        # Actualizar lags y rolling stats con el último dato conocido (de history)\n",
    "        last_known = history.iloc[-1]\n",
    "        for feature in features_to_lag:\n",
    "            current_step_features[f'{feature}_lag_1'] = last_known[feature]\n",
    "            # Para rolling stats, usamos los últimos datos de history\n",
    "            rolling_window = pd.concat()])[feature]\n",
    "            current_step_features[f'{feature}_rolling_mean_3'] = rolling_window.mean()\n",
    "            current_step_features[f'{feature}_rolling_std_3'] = rolling_window.std()\n",
    "\n",
    "        # Preprocesar la fila\n",
    "        current_step_numeric = current_step_features[X_full_numeric.columns]\n",
    "        current_step_imputed = imputer.transform(current_step_numeric)\n",
    "        current_step_scaled = scaler.transform(current_step_imputed)\n",
    "        \n",
    "        # Predecir\n",
    "        prediction = model.predict(current_step_scaled)\n",
    "        predictions.append(prediction)\n",
    "        \n",
    "        # Actualizar 'history' con la nueva predicción para el siguiente paso\n",
    "        new_row = current_step_features.copy()\n",
    "        new_row = prediction\n",
    "        history = pd.concat([history, new_row], ignore_index=True)\n",
    "        \n",
    "    return predictions\n",
    "\n",
    "# Generar pronósticos para ambos escenarios\n",
    "forecast_base = generate_forecast(final_model, df_featured[df_featured['country'] == 'Colombia'], future_df, scaler_final, imputer_final)\n",
    "forecast_optimistic = generate_forecast(final_model, df_featured[df_featured['country'] == 'Colombia'], future_df_optimistic, scaler_final, imputer_final)\n",
    "\n",
    "# --- Visualización de los resultados ---\n",
    "plt.figure(figsize=(18, 9))\n",
    "# Datos históricos\n",
    "plt.plot(df_featured[df_featured['country'] == 'Colombia']['year'], \n",
    "         df_featured[df_featured['country'] == 'Colombia'], \n",
    "         label='Histórico - SiB Colombia', color='black', marker='o')\n",
    "# Pronóstico Base\n",
    "plt.plot(future_years, forecast_base, label='Pronóstico Base (\"Business as Usual\")', color='blue', marker='x', linestyle='--')\n",
    "# Pronóstico Optimista\n",
    "plt.plot(future_years, forecast_optimistic, label='Pronóstico Optimista (↑ Gasto Educación)', color='green', marker='^', linestyle='--')\n",
    "\n",
    "plt.title('Pronóstico de Crecimiento de Publicación de Datos para SiB Colombia (2023-2030)', fontsize=16)\n",
    "plt.xlabel('Año', fontsize=12)\n",
    "plt.ylabel('Número de Registros Publicados (occurrenceCount_publisher)', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nProceso completado.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b2a6220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. IMPORTACI√ìN DE LIBRER√çAS Y CONFIGURACI√ìN INICIAL\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Preprocesamiento y modelado de Scikit-Learn\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split\n",
    "\n",
    "\n",
    "# Modelado de Deep Learning con TensorFlow/Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "# Configuraciones generales\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (15, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc576ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Datos filtrados para Colombia entre 2008 y 2022.\n",
      "Transformaci√≥n log1p aplicada al target.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 2. CARGA Y PREPARACI√ìN DE DATOS\n",
    "# =============================================================================\n",
    "url = \"https://raw.githubusercontent.com/rortizgeo/Maestria_CD_Proyecto-Aplicado/main/Data_final.csv\"\n",
    "Data_final = pd.read_csv(url)\n",
    "\n",
    "# 1. Filtrar por Pa√≠s\n",
    "Data_final_CO = Data_final[Data_final['country'] == 'Colombia'].copy()\n",
    "\n",
    "# 2. Filtrar por Rango de A√±os (AJUSTE CR√çTICO)\n",
    "START_YEAR = 2008 # Debido a que hay datos publicados a partir de este a√±o\n",
    "END_YEAR = 2022\n",
    "Data_final_CO = Data_final_CO[\n",
    "    (Data_final_CO['year'] >= START_YEAR) & \n",
    "    (Data_final_CO['year'] <= END_YEAR)\n",
    "].copy()\n",
    "print(f\"‚úÖ Datos filtrados para Colombia entre {START_YEAR} y {END_YEAR}.\")\n",
    "\n",
    "TARGET = 'occurrenceCount_publisher'\n",
    "Data_final_CO[TARGET] = np.log1p(Data_final_CO[TARGET])\n",
    "print(\"Transformaci√≥n log1p aplicada al target.\")\n",
    "\n",
    "columns_to_drop = ['Overall score', 'areas_protegidas', 'countryCode']\n",
    "Data_final_CO = Data_final_CO.drop(columns=columns_to_drop)\n",
    "\n",
    "features_total = ['country', 'year', 'ds', 'superficie_total_km2', 'gasto_RD_pib', 'efectividad_gobierno', 'uso_internet', 'pib_per_capita', 'Overall score', 'region', 'incomeLevel', 'occurrenceCount_publisher', 'gbif_member', 'ogp_membership', 'PC1', 'PC2', 'PC3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2dc2d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Secuencias LSTM generadas. Total de secuencias: 12.\n",
      "‚úÖ Las secuencias corresponden √∫nicamente a: Colombia\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 3. PREPARACI√ìN DE DATOS BASE PARA MODELOS (SIN ESCALADO GLOBAL)\n",
    "# =============================================================================\n",
    "\n",
    "# La funci√≥n se simplifica para procesar directamente un solo pa√≠s (df)\n",
    "def create_lstm_sequences_single_country(df, features, target, look_back=3):\n",
    "    \"\"\"\n",
    "    Genera secuencias LSTM exclusivamente para el DataFrame proporcionado (un solo pa√≠s).\n",
    "    \n",
    "    df: DataFrame YA filtrado para el pa√≠s y per√≠odo de inter√©s (ej., Data_final_CO).\n",
    "    \"\"\"\n",
    "    X_seq, y_seq, years, countries = [], [], [], []\n",
    "    \n",
    "    # Asegurar ordenamiento temporal\n",
    "    df_country = df.sort_values('year').copy()\n",
    "    \n",
    "    # 1. Preparar arrays num√©ricos (incluyendo la conversi√≥n a float)\n",
    "    # Se usan solo las features v√°lidas, excluyendo 'country'\n",
    "    X_country = df_country[features].values.astype(np.float32)\n",
    "    y_country = df_country[target].values.astype(np.float32)\n",
    "    years_country = df_country['year'].values\n",
    "    country_name = df_country['country'].iloc[0] # Obtener el nombre del pa√≠s (e.g., 'Colombia')\n",
    "    \n",
    "    # 2. Generar Secuencias\n",
    "    # Se requieren al menos (look_back + 1) filas para generar la primera secuencia\n",
    "    if len(X_country) > look_back:\n",
    "        for i in range(len(X_country) - look_back):\n",
    "            # X_seq: Secuencia de t a t + look_back - 1\n",
    "            X_seq.append(X_country[i:(i + look_back)])\n",
    "            # y_seq: El valor a predecir en el tiempo t + look_back\n",
    "            y_seq.append(y_country[i + look_back])\n",
    "            years.append(years_country[i + look_back])\n",
    "            countries.append(country_name)\n",
    "            \n",
    "    return np.array(X_seq), np.array(y_seq), np.array(years), np.array(countries)\n",
    "\n",
    "# Definici√≥n de variables para la funci√≥n\n",
    "\n",
    "# Columnas a excluir del input X (ya que son el target o IDs de control)\n",
    "columnas_a_excluir_lstm = ['country', TARGET, 'occurrenceCount_publisher', 'ds'] \n",
    "\n",
    "# Crear la lista final de features num√©ricos a usar en la LSTM\n",
    "# Se usa features_total (que contiene todas las variables, incluyendo las ya convertidas)\n",
    "features_lstm = [f for f in features_total if f not in columnas_a_excluir_lstm]\n",
    "\n",
    "# Asegurar que solo queden las columnas presentes en el DF filtrado\n",
    "features_lstm = [f for f in features_lstm if f in Data_final_CO.columns] \n",
    "\n",
    "look_back = 3\n",
    "\n",
    "# LLAMADA CORREGIDA: Utilizar Data_final_CO y la funci√≥n simplificada\n",
    "X_seq, y_seq, years_seq, countries_seq = create_lstm_sequences_single_country(\n",
    "    Data_final_CO, \n",
    "    features=features_lstm, \n",
    "    target=TARGET, \n",
    "    look_back=look_back\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Secuencias LSTM generadas. Total de secuencias: {len(X_seq)}.\")\n",
    "print(f\"‚úÖ Las secuencias corresponden √∫nicamente a: {countries_seq[0] if len(countries_seq) > 0 else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6def6f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# M√ìDULO DE ENTRENAMIENTO FINAL (Solo para Colombia)\n",
    "# =============================================================================\n",
    "\n",
    "# üß† LSTM (Par√°metros Fijos del Mejor Fold)\n",
    "# Esta lista ahora representa el √∫nico set de par√°metros a utilizar.\n",
    "LSTM_FINAL_PARAMS = {\n",
    "    \"units\": 50, \n",
    "    \"dropout\": 0.3, \n",
    "    \"epochs\": 30, \n",
    "    \"batch_size\": 32, \n",
    "    \"learning_rate\": 0.005, \n",
    "    \"lstm_activation\": \"tanh\"\n",
    "}\n",
    "\n",
    "def train_final_lstm_model(X_train, y_train, look_back, final_params=LSTM_FINAL_PARAMS):\n",
    "    \"\"\"\n",
    "    Entrena el modelo LSTM final utilizando los hiperpar√°metros preseleccionados\n",
    "    del mejor fold de validaci√≥n cruzada.\n",
    "    \n",
    "    Args:\n",
    "        X_train (np.array): Secuencias 3D de features escalados de Colombia.\n",
    "        y_train (np.array): Target escalado de Colombia.\n",
    "        look_back (int): Longitud de la secuencia de entrada.\n",
    "        final_params (dict): Hiperpar√°metros √≥ptimos preseleccionados.\n",
    "        \n",
    "    Returns:\n",
    "        tf.keras.models.Sequential: El modelo LSTM final entrenado.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Usamos los par√°metros fijos\n",
    "    params = final_params \n",
    "    \n",
    "    # Dividir una peque√±a porci√≥n para Early Stopping (Mejora la robustez del entrenamiento)\n",
    "    # Se usa test_size=0.05 (5%) para maximizar los datos de entrenamiento\n",
    "    X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.05, random_state=42, shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Inicializar Early Stopping\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', patience=5, verbose=1, restore_best_weights=True)\n",
    "\n",
    "    # 1. Definici√≥n de la Arquitectura\n",
    "    model = Sequential([\n",
    "        LSTM(params[\"units\"], \n",
    "             activation=params[\"lstm_activation\"], \n",
    "             input_shape=(look_back, X_train.shape[2])), \n",
    "        Dropout(params[\"dropout\"]), \n",
    "        Dense(1) \n",
    "    ])\n",
    "    \n",
    "    # 2. Compilaci√≥n y Entrenamiento\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=params['learning_rate'])\n",
    "    model.compile(optimizer=optimizer, loss=\"mae\")\n",
    "    \n",
    "    print(f\"\\nüöÄ Iniciando entrenamiento final con {len(X_train_final)} muestras...\")\n",
    "    \n",
    "    model.fit(X_train_final, y_train_final, \n",
    "              epochs=params[\"epochs\"], \n",
    "              batch_size=params[\"batch_size\"], \n",
    "              verbose=0,\n",
    "              validation_data=(X_val, y_val),\n",
    "              callbacks=[es])\n",
    "    \n",
    "    # No hay necesidad de retornar best_params ya que son fijos.\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a433d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6. M√âTRICAS\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def smape(y_true, y_pred, eps=1e-8):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred)) + eps\n",
    "    return np.mean(2.0 * np.abs(y_pred - y_true) / denom) * 100\n",
    "\n",
    "def mape(y_true, y_pred, eps=1e-8):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / (y_true + eps))) * 100\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    # CORRECCI√ìN 3: Revertir la transformaci√≥n log1p\n",
    "    y_true_orig = np.expm1(y_true)\n",
    "    y_pred_orig = np.expm1(y_pred)\n",
    "    \n",
    "    return {\n",
    "        \"MAE\": mean_absolute_error(y_true_orig, y_pred_orig),\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_true_orig, y_pred_orig)),\n",
    "        \"R2\": r2_score(y_true_orig, y_pred_orig),\n",
    "        \"MAPE\": mape(y_true_orig, y_pred_orig),\n",
    "        \"SMAPE\": smape(y_true_orig, y_pred_orig)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b002c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Datos cargados y filtrados. Filas hist√≥ricas para entrenamiento: 15\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# M√ìDULO 1: CONFIGURACI√ìN, LIBRER√çAS Y CARGA DE DATOS\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Librer√≠as de Machine Learning/Deep Learning\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# --- CONSTANTES GLOBALES Y DE MODELO ---\n",
    "TARGET_RAW = 'occurrenceCount_publisher'\n",
    "TARGET = f'{TARGET_RAW}_log1p' \n",
    "COUNTRY_CODE = 'Colombia'\n",
    "SEQUENCE_LENGTH = 3      # Look-back\n",
    "START_YEAR = 2008\n",
    "END_YEAR = 2022\n",
    "YEARS_TO_FORECAST = 8    # Proyectar de 2023 a 2030 (8 a√±os)\n",
    "\n",
    "# Lista de variables NUM√âRICAS que ser√°n input de la LSTM\n",
    "FEATURES_NUMERIC_FOR_SCALING = [\n",
    "    \"year\", \"superficie_total_km2\", \"gasto_RD_pib\", \"efectividad_gobierno\", \n",
    "    \"uso_internet\", \"pib_per_capita\", \"Overall score\", \"region\", \"incomeLevel\", \n",
    "    \"gbif_member\", \"ogp_membership\", \"PC1\", \"PC2\", \"PC3\"\n",
    "]\n",
    "\n",
    "# Par√°metros √ìptimos de LSTM (Fijos, extra√≠dos del mejor Fold)\n",
    "LSTM_PARAMS = {'units': 50, 'dropout': 0.3, 'epochs': 30, 'batch_size': 32, 'learning_rate': 0.001, 'lstm_activation': 'tanh'}\n",
    "\n",
    "# --- CARGA Y FILTRADO DE DATOS ---\n",
    "try:\n",
    "    # 1. Cargar datos globales\n",
    "    df_global = pd.read_csv(\"Data_final.csv\")\n",
    "    \n",
    "    # 2. Aplicar Transformaci√≥n y Tipado\n",
    "    df_global[TARGET] = np.log1p(df_global[TARGET_RAW])\n",
    "    df_global['year'] = df_global['year'].astype(int) \n",
    "    \n",
    "    # 3. Filtrar por Pa√≠s y Rango de A√±os (2008-2022)\n",
    "    df_colombia_all = df_global[df_global['country'] == COUNTRY_CODE].copy()\n",
    "    \n",
    "    df_colombia_filtered = df_colombia_all[\n",
    "        (df_colombia_all['year'] >= START_YEAR) & \n",
    "        (df_colombia_all['year'] <= END_YEAR)\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"‚úÖ Datos cargados y filtrados. Filas hist√≥ricas para entrenamiento: {len(df_colombia_filtered)}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå ERROR: No se encontr√≥ Data_final.csv.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7ddbeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# M√ìDULO 2: FUNCIONES CORE (Secuenciaci√≥n y Preparaci√≥n)\n",
    "# =============================================================================\n",
    "\n",
    "def create_sequences(X, y, time_steps):\n",
    "    \"\"\"Convierte arrays X e Y en secuencias 3D para LSTM.\"\"\"\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(X[i:(i + time_steps)])\n",
    "        ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "def build_lstm_model(timesteps, features, params):\n",
    "    \"\"\"Construye el modelo LSTM con los hiperpar√°metros √≥ptimos.\"\"\"\n",
    "    model = Sequential([\n",
    "        LSTM(params.get('units'), activation=params.get('lstm_activation'), \n",
    "             input_shape=(timesteps, features), return_sequences=False),\n",
    "        Dropout(params.get('dropout')),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=params.get('learning_rate'))\n",
    "    model.compile(optimizer=optimizer, loss='mae')\n",
    "    return model\n",
    "\n",
    "def prepare_data_lstm(df_global, df_colombia_filtered, features_numeric, target_col, sequence_length):\n",
    "    \"\"\"Ajusta escaladores en el GLOBAL, los aplica a los datos de Colombia (2008-2022), y genera secuencias.\"\"\"\n",
    "    \n",
    "    # 1. Ajustar Escaladores en el conjunto GLOBAL (para replicar el contexto de entrenamiento)\n",
    "    y_global = df_global[[target_col]].values.astype(np.float32)\n",
    "    y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    y_scaler.fit(y_global)\n",
    "    \n",
    "    X_global = df_global[features_numeric].values.astype(np.float32) \n",
    "    X_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    X_scaler.fit(X_global)\n",
    "    \n",
    "    # 2. Limpieza y Aplicaci√≥n de Escalado a COLOMBIA\n",
    "    df_train_raw = df_colombia_filtered.dropna(subset=features_numeric + [target_col]).copy()\n",
    "    \n",
    "    # Aplicar escalado a X y Y de Colombia\n",
    "    y_colombia_scaled = y_scaler.transform(df_train_raw[[target_col]].values)\n",
    "    X_colombia_scaled = X_scaler.transform(df_train_raw[features_numeric].values)\n",
    "    \n",
    "    # 3. Generar Secuencias\n",
    "    X_seq, y_seq = create_sequences(X_colombia_scaled, y_colombia_scaled, sequence_length)\n",
    "    X_raw_scaled = X_colombia_scaled\n",
    "    \n",
    "    return X_seq, y_seq, X_raw_scaled, y_scaler, X_scaler, df_train_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb692b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# M√ìDULO 3: PRON√ìSTICO AUTOREGRESIVO (EXECUCI√ìN FINAL CORREGIDA)\n",
    "# =============================================================================\n",
    "\n",
    "def execute_lstm_forecast_final(df_global, df_colombia_filtered, sequence_length, lstm_params, years_to_forecast):\n",
    "    \n",
    "    # 1. Preparaci√≥n de Datos (Obtiene escaladores y secuencias)\n",
    "    X_seq, y_seq, X_raw_scaled, y_scaler, X_scaler, df_train_raw = prepare_data_lstm(\n",
    "        df_global, df_colombia_filtered, FEATURES_NUMERIC_FOR_SCALING, TARGET, sequence_length\n",
    "    )\n",
    "    \n",
    "    # 2. Entrenamiento Final\n",
    "    timesteps = X_seq.shape[1]\n",
    "    n_features = X_seq.shape[2]\n",
    "    \n",
    "    model = build_lstm_model(timesteps, n_features, lstm_params)\n",
    "    es = EarlyStopping(monitor='loss', patience=5, verbose=0, mode='min')\n",
    "    \n",
    "    model.fit(X_seq, y_seq,\n",
    "              epochs=lstm_params.get('epochs', 30), \n",
    "              batch_size=lstm_params.get('batch_size', 32), \n",
    "              verbose=0, callbacks=[es])\n",
    "    print(f\"‚úÖ Modelo LSTM reentrenado en {len(X_seq)} secuencias.\")\n",
    "\n",
    "    # 3. Inicializaci√≥n del Bucle Autoregresivo\n",
    "    last_historical_year = df_train_raw['year'].max() # √öltimo a√±o real (2022)\n",
    "    end_year = last_historical_year + years_to_forecast # Fin del pron√≥stico (2030)\n",
    "    \n",
    "    projected_rows_list = [] \n",
    "    last_sequence_scaled = X_raw_scaled[-sequence_length:] \n",
    "\n",
    "    for year in range(last_historical_year + 1, end_year + 1):\n",
    "        \n",
    "        # 3.1. Definir los FEATURES EX√ìGENOS FUTUROS (Supuesto de Continuidad)\n",
    "        X_next_raw = df_train_raw[FEATURES_NUMERIC_FOR_SCALING].iloc[-1].copy()\n",
    "        \n",
    "        # 3.2. Escalar X y preparar la secuencia\n",
    "        X_next_scaled = X_scaler.transform(X_next_raw.values.reshape(1, -1))\n",
    "\n",
    "        # 3.3. Actualizar la Secuencia de Entrada y predecir\n",
    "        X_forecast_seq = np.vstack([last_sequence_scaled[1:], X_next_scaled])\n",
    "        X_forecast_seq_3D = X_forecast_seq.reshape(1, sequence_length, n_features)\n",
    "        \n",
    "        predicted_scaled = model.predict(X_forecast_seq_3D, verbose=0)[0]\n",
    "        predicted_log1p = y_scaler.inverse_transform(predicted_scaled.reshape(-1, 1))[0, 0]\n",
    "        predicted_records = np.expm1(predicted_log1p)\n",
    "        \n",
    "        # 3.4. Crear la nueva fila de datos (solo las columnas esenciales)\n",
    "        new_row_data = {\n",
    "            'year': year,\n",
    "            'country': country_code,\n",
    "            TARGET_RAW: predicted_records # Valor de la predicci√≥n futura\n",
    "        }\n",
    "        \n",
    "        projected_rows_list.append(new_row_data)\n",
    "        \n",
    "        # 3.5. Actualizar la secuencia del estado para la pr√≥xima iteraci√≥n\n",
    "        last_sequence_scaled = X_forecast_seq \n",
    "\n",
    "    # 4. Concatenaci√≥n Final y Formato (L√≥gica Corregida)\n",
    "    \n",
    "    # 4.1 Preparar DF de proyecci√≥n solo con los resultados\n",
    "    df_projected_only = pd.DataFrame(projected_rows_list)\n",
    "    \n",
    "    # 4.2 Preparar el DF hist√≥rico para la uni√≥n (solo las columnas de salida)\n",
    "    df_historical_view = df_train_raw[['year', TARGET_RAW]].copy()\n",
    "    \n",
    "    # 4.3 Concatenar Hist√≥rico (2008-2022) y Proyecci√≥n (2023-2030)\n",
    "    df_final = pd.concat([df_historical_view, df_projected_only], ignore_index=True)\n",
    "    \n",
    "    # 4.4 Columna de valores REALES (solo para a√±os <= 2022)\n",
    "    df_final['historical_records'] = df_final[TARGET_RAW].where(df_final['year'] <= last_historical_year).round(0)\n",
    "    \n",
    "    # 4.5 Columna de PRON√ìSTICO (valores proyectados + valores hist√≥ricos para la l√≠nea de inicio)\n",
    "    df_final['predicted_records'] = df_final[TARGET_RAW].where(df_final['year'] > last_historical_year)\n",
    "    \n",
    "    # Rellenar la serie de predicci√≥n con los valores hist√≥ricos para la continuidad del plot\n",
    "    df_final['predicted_records'] = df_final['predicted_records'].fillna(df_final['historical_records']).round(0)\n",
    "\n",
    "    # 4.6 Limpiar columnas y retornar\n",
    "    df_final = df_final.drop(columns=[TARGET_RAW], errors='ignore')\n",
    "    \n",
    "    return df_final, last_historical_year"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
